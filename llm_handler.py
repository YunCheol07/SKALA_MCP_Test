"""
A.X 4.0 Light ëª¨ë¸ í•¸ë“¤ëŸ¬ (í…ìŠ¤íŠ¸ ì „ìš©)
ì•ˆì •ì ì¸ í…ìŠ¤íŠ¸ ì „ìš© ë²„ì „ ì‚¬ìš©
"""
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from typing import List, Dict, Optional
import config


class LLMHandler:
    """A.X 4.0 Light ëª¨ë¸ì„ ê´€ë¦¬í•˜ëŠ” í•¸ë“¤ëŸ¬ í´ë˜ìŠ¤ (í…ìŠ¤íŠ¸ ì „ìš©)"""
    
    def __init__(self, model_name: str = config.MODEL_NAME):
        """
        Args:
            model_name: í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ ì´ë¦„
        """
        print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        print(f"   ë””ë°”ì´ìŠ¤: {config.DEVICE}")
        
        self.device = config.DEVICE
        self.model_name = model_name
        
        # ëª¨ë¸ ë¡œë“œ
        dtype = torch.bfloat16 if self.device == "cuda" else torch.float32
        
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=dtype,
            device_map=config.MODEL_CONFIG["device_map"],
            cache_dir=config.MODEL_CACHE_DIR,
        )
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=config.MODEL_CACHE_DIR,
        )
        
        self.model.eval()
        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    def generate_response(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        max_new_tokens: int = config.GENERATION_CONFIG["max_new_tokens"],
        temperature: float = config.GENERATION_CONFIG["temperature"],
    ) -> str:
        """
        í…ìŠ¤íŠ¸ ì…ë ¥ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        
        Args:
            prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒì‚¬í•­)
            max_new_tokens: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜
            temperature: ìƒì„± ì˜¨ë„
            
        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸ ì‘ë‹µ
        """
        # ë©”ì‹œì§€ êµ¬ì„±
        messages = []
        
        if system_prompt:
            messages.append({
                "role": "system",
                "content": system_prompt
            })
        
        messages.append({
            "role": "user",
            "content": prompt
        })
        
        # í† í¬ë‚˜ì´ì¦ˆ - tokenize=Falseë¡œ ë¨¼ì € í…ìŠ¤íŠ¸ë§Œ ìƒì„±
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜
        input_ids = self.tokenizer(
            text,
            return_tensors="pt",
            add_special_tokens=False
        ).input_ids.to(self.device)
        
        # ìƒì„±
        with torch.no_grad():
            output_ids = self.model.generate(
                input_ids,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=config.GENERATION_CONFIG["top_p"],
                top_k=config.GENERATION_CONFIG["top_k"],
                do_sample=config.GENERATION_CONFIG["do_sample"],
            )
        
        # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì œì™¸
        len_input_prompt = input_ids.shape[1]
        generated_ids = output_ids[0][len_input_prompt:]
        
        # ë””ì½”ë”©
        response = self.tokenizer.decode(
            generated_ids,
            skip_special_tokens=True
        )
        
        return response.strip()
    
    def chat(
        self,
        messages: List[Dict[str, str]],
        max_new_tokens: int = config.GENERATION_CONFIG["max_new_tokens"],
    ) -> str:
        """
        ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        
        Args:
            messages: ëŒ€í™” ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
                ì˜ˆ: [{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}]
            max_new_tokens: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜
            
        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸ ì‘ë‹µ
        """
        # í† í¬ë‚˜ì´ì¦ˆ - tokenize=Falseë¡œ ë¨¼ì € í…ìŠ¤íŠ¸ë§Œ ìƒì„±
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜
        input_ids = self.tokenizer(
            text,
            return_tensors="pt",
            add_special_tokens=False
        ).input_ids.to(self.device)
        
        with torch.no_grad():
            output_ids = self.model.generate(
                input_ids,
                max_new_tokens=max_new_tokens,
                temperature=config.GENERATION_CONFIG["temperature"],
                top_p=config.GENERATION_CONFIG["top_p"],
                top_k=config.GENERATION_CONFIG["top_k"],
                do_sample=config.GENERATION_CONFIG["do_sample"],
            )
        
        len_input_prompt = input_ids.shape[1]
        generated_ids = output_ids[0][len_input_prompt:]
        
        response = self.tokenizer.decode(
            generated_ids,
            skip_special_tokens=True
        )
        
        return response.strip()


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    print("=" * 50)
    print("A.X 4.0 Light ëª¨ë¸ í…ŒìŠ¤íŠ¸ (í…ìŠ¤íŠ¸ ì „ìš©)")
    print("=" * 50)
    
    # LLM í•¸ë“¤ëŸ¬ ì´ˆê¸°í™”
    llm = LLMHandler()
    
    # í…ìŠ¤íŠ¸ ì „ìš© í…ŒìŠ¤íŠ¸
    print("\nğŸ“ í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸ 1:")
    response = llm.generate_response(
        prompt="ì¸ê³µì§€ëŠ¥ì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ì¤˜.",
        system_prompt="ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
    )
    print(f"ì‘ë‹µ: {response}")
    
    print("\nğŸ“ í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸ 2:")
    response = llm.generate_response(
        prompt="MCP(Model Context Protocol)ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜.",
    )
    print(f"ì‘ë‹µ: {response}")
    
    print("\nğŸ“ ëŒ€í™” í˜•ì‹ í…ŒìŠ¤íŠ¸:")
    messages = [
        {"role": "system", "content": "ë‹¹ì‹ ì€ íŒŒì´ì¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
        {"role": "user", "content": "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì€?"},
    ]
    response = llm.chat(messages)
    print(f"ì‘ë‹µ: {response}")
    
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")